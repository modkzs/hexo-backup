title: 神经网络 CNN
tags: [Neural Network]
date: 2016-01-27 12:54:03
categories: [ml]
---
脑洞大开的机器学习江湖史——CNN横空出世，图像识别天下一统
<!--more-->
虽然一代天才BP让神经网络摆脱了机器学习江湖里面龙套门派的角色，但是同一时间内SVM等的出世大大抹掉了BP的风头。虽然对于内功的修改如权值共享等一直在继续，但是一直效果不佳。直到某党上线，导致雾霾漫天，啊不对，一种叫大数据的东西出现，而且计算能力的提升导致整个江湖的环境大大改变，CNN才一跃而起，一统CV的天下，不仅如此，还大有像NLP进军的趋势。

CNN是一种典型的通过权值共享减少训练难度的模型。因为图像的特点导致其小部分的特征一致，因此可以使用权值共享
我们首先从最简单的BP开始。我们在处理一张200*200的像素图时(如下图所示：)
![](http://ww1.sinaimg.cn/mw690/9dec4451gw1f0vpnykx26j20lc0k60vz.jpg)
（据说这张被所有做图像的人玩烂的图片来自花花公子╮(￣▽￣")╭）
那么就是4W\*4W的权重，这显然不好搞啊。那就像前面说的一样，图像的局部感觉差不多么，那我们简单粗暴的去降维！
![](http://ww4.sinaimg.cn/mw690/9dec4451gw1f0vpsft99vj20l409e0ub.jpg)
上面的图片中，最简单的理解就是对原图片进行降维。当然了，$W_A$ 和$W_B$当然是一样的了。当然为了对这种方式有一种合理的解释，把这个和什么视觉生理学硬扯起来，然后还起个局部连接的名字(￣ ‘i ￣;)这本身就是神经网络的一层了，你想用啥函数去做都行，这里激活函数也不叫激活函数了，叫滤波器或者卷积核。当然了，用一种方式做降维多low B，拉出去都不好意思见人的。所以一般会采用不同的激活函数进行这种操作，当然了，人家这时候就不叫激活函数了，人家叫卷积，立马高大上起来了好么！
![](http://ww1.sinaimg.cn/mw690/9dec4451gw1f0vq1pben2j21120cmdj9.jpg)
比如上图，就用了4种卷积核去做卷积。当然了，对于为什么这么做，人家说了，可以认为是一个卷积核提取一个特征，多个卷积核提取多个特征。不就是强行提高B格么=。=

还有一种办法叫pooling。pooling就比较low B了。简单来说就是认为一个 $n\times n$ 的像素块为一个区域，一般取最大值或者用平均值进行pooling（其实和卷积差不多，只是函数是最大值或者平局值而已）。但是这样就会出现问题，平均还好说，取极值的话你的误差怎么反向传导呢？据说比如说卷积核的结果是[a,b,c,d]，第2个最大，可以认为是卷积核和向量[0,1,0,0]做内积。这样就可以方向传导了。当然我没有试过，有兴趣你可以试一试，反正我这辈子都不想再去写神经网络了，每写一次至少让我死一般脑细胞在写导数上，而且让我都有一种自己是个SB的错觉（说不定不是错觉(。>︿<)）

OK，讲到这里，基本的CNN就完事了。作为一只伪nlp狗，虽然cnn已经用在nlp上，但是目前没有细看cnn的想法，所以也没有什么扩展的。最后看下一个整体的例子吧
![](http://ww1.sinaimg.cn/mw690/9dec4451gw1f0vti2erbaj20vw09ymyq.jpg)
上图中首先用卷积层进行卷积，卷基层使用了4个卷积函数。然后第二层为pooling层，用了一个2x2的窗口，然后得到第三层。当然第三层处理的就是立体数据，也就是4x4x4的维度的数据（tensor的处理和以前基本一样，用   {% math %}y_j^L=f\big(\sum_i(y_i^{L-1}\times K_{ij})\big){% endmath %}  ），然后用了7个卷积去做卷积层。。。。逐层往上就行了
