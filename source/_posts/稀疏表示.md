title: 稀疏表示
tags: [machine learning, Neural Network]
date: 2016-02-22 13:58:49
categories: [ml]
---
参数的稀疏性在machine learning，尤其是deep learning中是一个很重要的话题。故名思意，稀疏表示就是指训练出的参数是稀疏的，即包含很多的0
<!--more-->
# 稀疏表示的方法
目前接触到可以进行稀疏表示的方法比较少，大部分都是从deep learning中发现的。
## ReLU
神经网络的一种激活函数，可以做到稀疏表示。并不清楚原理。ReLU形式如下：
$$
f(x) = max(0,x+N(0,1))\\
f(x) = log(1+e^x)
$$
ReLU的详细描述可以去看我的另一片[文章](http://modkzs.github.io/2016/01/23/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-ReLU%EF%BC%8Ccross-entropy-and-softmax/)。
## L1
即为损失函数添加L1惩罚系数，古已有之。对于L1为什么会造成稀疏表示，有一副广为人知的图片：
![](http://ww3.sinaimg.cn/large/9dec4451gw1f1877lohmtj20hb0a7q60.jpg)
简单的描述了L1为什么会导致稀疏表示。当然对于L1导致稀疏表示的数学证明，也可以去看我的文章
# 稀疏表示的优点
根据Bengio的Deep Sparse Rectifier Neural Networks，稀疏表示有如下优点
## Information disentangling
对于density representation，input中任何一个值的改变都可能导致整个representation vector的巨大改变，而稀疏表示对于input的微小改变表现出较高的鲁棒性（个人感觉这个L2也可以做到）
## Efficient variable-size representation
不同的数据所含有的信息量会有一定差异，而稀疏表示可以较大的改变激活神经元的数量。稀疏表示使得模型可以针对不同的输入较好地控制维度数量。
## Linear separability
稀疏表示更可能线性可分，或者更容易被一些非线性方法分类。（我觉得这里的意思应该是稀疏表示的神经网络提取的feature更容易被分类）
## Distributed but sparse
首先需要理解Distributed representation
### Distributed and local representation
对于distributed representation，英文的解释如下：

> Distributed representations are compact, dense and low dimensional representation, with each factor in the representation representing some distinct informative property.

我个人的理解是distributed representation中每一维的数据都表示了一定的区域信息。举个例子吧，我们有8个点，分别为[1,2,3...8]，如果我们采用二进制，只需要3个feature就可以划分8个点，如果我们采用十进制，需要8个feature。二进制的表示就是一种Distributed representation

而dense distributed representation是最冗余(richest)的表示，而稀疏表示自然能大大提高效率。
